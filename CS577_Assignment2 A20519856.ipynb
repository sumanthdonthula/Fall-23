{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_hx8SJJR4-"
      },
      "source": [
        "# Assignment 2 - Recurrent Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AiWTVDf7cZ2"
      },
      "source": [
        "## Programming (Full points: 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYkmezTbc-Ed"
      },
      "source": [
        "In this assignment, our goal is to use PyTorch to implement Recurrent Neural Networks (RNN) for sentiment analysis task. Sentiment analysis is to classify sentences (input) into certain sentiments (output labels), which includes positive, negative and neutral.\n",
        "\n",
        "We will use a benckmark dataset, SST, for this assignment.\n",
        "* we download the SST dataset from torchtext package, and do some preprocessing to build vocabulary and split the dataset into training/validation/test sets. You don't need to modify the code in this step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IKBkronF5me",
        "outputId": "8e9383ae-a104-4bce-c70f-e53713268122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0. in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0.) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0.) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0.) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0.) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0.) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0.) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0.) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0.) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0.) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0.) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0.) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0.) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0.) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0.) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0.) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0.) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0.) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0.) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0.) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0.) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.6.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vrv5zx6lc-Ed"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "# load data splits\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
        "\n",
        "# build dictionary\n",
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "\n",
        "# build iterators\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-O1SBcc-Eg"
      },
      "source": [
        "* define the training and evaluation function in the cell below.\n",
        "### (25 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d1AkdwOmc-Eh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#Train Model Method\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs, labels = batch.text.to(device), batch.label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n",
        "#Evaluate Model Method\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs, labels = batch.text.to(device), batch.label.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qEZaW8bc-Ei"
      },
      "source": [
        "* build a RNN model for sentiment analysis in the cell below.\n",
        "We have provided several hyperparameters we needed for building the model, including vocabulary size (vocab_size), the word embedding dimension (embedding_dim), the hidden layer dimension (hidden_dim), the number of layers (num_layers) and the number of sentence labels (label_size). Please fill in the missing codes, and implement a RNN model.\n",
        "### (40 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TGBiaFijc-Ei"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx,num_layers):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding Layers\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # Using LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True  # Use LSTM, set batch_first=True\n",
        "        )\n",
        "\n",
        "        # Adding a linear layer for classification.\n",
        "        self.fc = nn.Linear(self.hidden_dim, self.label_size)\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        #initial hidden state.\n",
        "        return None\n",
        "\n",
        "    def forward(self, text):\n",
        "        #forward function of the model.\n",
        "\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "\n",
        "        # Get the output of the last time step via LSTM\n",
        "        output = lstm_out[:, -1, :]\n",
        "\n",
        "        # Linear layer for classification\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ldxYfovc-Ei"
      },
      "source": [
        "* train the model and compute the accuracy in the cell below.\n",
        "### (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSmHzfngc-Ej",
        "outputId": "d150a32c-1164-4847-e8fd-7019fd349702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 1.0520, Train Accuracy: 41.33%\n",
            "Validation Loss: 1.0828, Validation Accuracy: 40.42%\n",
            "Epoch 2/20\n",
            "Train Loss: 1.0462, Train Accuracy: 42.03%\n",
            "Validation Loss: 1.0671, Validation Accuracy: 40.15%\n",
            "Epoch 3/20\n",
            "Train Loss: 1.0448, Train Accuracy: 41.65%\n",
            "Validation Loss: 1.1326, Validation Accuracy: 40.69%\n",
            "Epoch 4/20\n",
            "Train Loss: 1.0347, Train Accuracy: 43.83%\n",
            "Validation Loss: 1.0779, Validation Accuracy: 46.32%\n",
            "Epoch 5/20\n",
            "Train Loss: 1.0088, Train Accuracy: 49.18%\n",
            "Validation Loss: 1.0773, Validation Accuracy: 46.87%\n",
            "Epoch 6/20\n",
            "Train Loss: 0.9508, Train Accuracy: 56.04%\n",
            "Validation Loss: 1.0442, Validation Accuracy: 50.32%\n",
            "Epoch 7/20\n",
            "Train Loss: 0.8715, Train Accuracy: 62.61%\n",
            "Validation Loss: 1.0181, Validation Accuracy: 53.59%\n",
            "Epoch 8/20\n",
            "Train Loss: 0.7761, Train Accuracy: 67.60%\n",
            "Validation Loss: 1.0137, Validation Accuracy: 54.86%\n",
            "Epoch 9/20\n",
            "Train Loss: 0.7006, Train Accuracy: 71.36%\n",
            "Validation Loss: 1.0267, Validation Accuracy: 55.86%\n",
            "Epoch 10/20\n",
            "Train Loss: 0.6219, Train Accuracy: 74.63%\n",
            "Validation Loss: 1.0454, Validation Accuracy: 55.40%\n",
            "Epoch 11/20\n",
            "Train Loss: 0.5640, Train Accuracy: 77.56%\n",
            "Validation Loss: 1.0510, Validation Accuracy: 56.68%\n",
            "Epoch 12/20\n",
            "Train Loss: 0.5127, Train Accuracy: 80.63%\n",
            "Validation Loss: 1.1135, Validation Accuracy: 53.59%\n",
            "Epoch 13/20\n",
            "Train Loss: 0.4679, Train Accuracy: 83.40%\n",
            "Validation Loss: 1.1061, Validation Accuracy: 53.50%\n",
            "Epoch 14/20\n",
            "Train Loss: 0.4290, Train Accuracy: 85.30%\n",
            "Validation Loss: 1.1513, Validation Accuracy: 54.41%\n",
            "Epoch 15/20\n",
            "Train Loss: 0.3883, Train Accuracy: 87.18%\n",
            "Validation Loss: 1.1404, Validation Accuracy: 55.68%\n",
            "Epoch 16/20\n",
            "Train Loss: 0.3527, Train Accuracy: 89.07%\n",
            "Validation Loss: 1.1737, Validation Accuracy: 55.95%\n",
            "Epoch 17/20\n",
            "Train Loss: 0.3214, Train Accuracy: 90.17%\n",
            "Validation Loss: 1.1774, Validation Accuracy: 55.40%\n",
            "Epoch 18/20\n",
            "Train Loss: 0.2874, Train Accuracy: 91.83%\n",
            "Validation Loss: 1.2249, Validation Accuracy: 55.86%\n",
            "Epoch 19/20\n",
            "Train Loss: 0.2572, Train Accuracy: 92.85%\n",
            "Validation Loss: 1.2629, Validation Accuracy: 55.77%\n",
            "Epoch 20/20\n",
            "Train Loss: 0.2491, Train Accuracy: 93.07%\n",
            "Validation Loss: 1.3092, Validation Accuracy: 53.68%\n",
            "\n",
            "Test Accuracy: 57.92%\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "label_size = 3 # Number of output labels\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx,num_layers)\n",
        "model.to(device)  # Move the model to GPU if available\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_train_loss = 0.0\n",
        "    total_train_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        inputs, labels = batch.text.to(device), batch.label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average training loss and accuracy for the epoch\n",
        "    average_train_loss = total_train_loss / len(train_iter)\n",
        "    average_train_accuracy = 100 * total_train_accuracy / len(train_data)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_accuracy = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_iter:\n",
        "            inputs, labels = batch.text.to(device), batch.label.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average validation loss and accuracy for the epoch\n",
        "    average_val_loss = total_val_loss / len(val_iter)\n",
        "    average_val_accuracy = 100 * total_val_accuracy / len(val_data)\n",
        "\n",
        "    # Print training and validation results for the epoch\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {average_train_loss:.4f}, Train Accuracy: {average_train_accuracy:.2f}%\")\n",
        "    print(f\"Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {average_val_accuracy:.2f}%\")\n",
        "\n",
        "#evaluate on the testing set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        inputs, labels = batch.text.to(device), batch.label.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_test_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate and print the test accuracy\n",
        "test_accuracy = 100 * total_test_accuracy / len(test_data)\n",
        "print(\"\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: with the Initial RNN and parameters the model is overfitting and we need to increase the Accuracy on Test and Validation sets, The Model is Overfitting**"
      ],
      "metadata": {
        "id": "wp_htQQ-RaFn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaOhyRClc-Ek"
      },
      "source": [
        "* try to train a model with better accuracy in the cell below. For example, you can use different optimizers such as SGD and Adam. You can also compare different hyperparameters and model size.\n",
        "### (15 points), to obtain FULL point in this problem, the accuracy needs to be higher than 70%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Increase the accuracy and performance on Test and Validation Dataset. We have made the following changes\n",
        "\n",
        "1.   Remove Stop Words using NLTK Package\n",
        "2.   Used pretrained embeddings like Glove\n",
        "3.   Increased the batch size\n",
        "4.   Used Bidirectional RNN\n",
        "5.   Used a scheduler with Optimization Function\n",
        "6.   Used Dropout and LL1 Regularization\n",
        "\n"
      ],
      "metadata": {
        "id": "qIbaQscGRgzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GloVe vectors\n",
        "TEXT.build_vocab(train_data, vectors=\"glove.6B.300d\", unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK's stopwords data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "# Load data splits\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
        "\n",
        "# Build dictionary\n",
        "TEXT.build_vocab(train_data, vectors=\"glove.6B.300d\", unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# Get the English stop words from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to remove stop words from a list of tokens\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Apply the stop word removal function during tokenization\n",
        "TEXT.preprocessing = data.Pipeline(remove_stopwords)\n",
        "\n",
        "# Build iterators\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niRHmR_qmOYZ",
        "outputId": "87904f7d-7734-48bc-8323-32644723242f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BetterRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, num_layers, dropout_prob):\n",
        "        super(BetterRNN, self).__init__()  # Note the parentheses after super\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_prob           # Dropout for regularization\n",
        "        )\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(2 * num_layers * hidden_dim, label_size)  # Output layer\n",
        "\n",
        "    def forward(self, input_sentences):\n",
        "        # Input Embedding\n",
        "        embedded = self.embedding(input_sentences)\n",
        "\n",
        "        # LSTM Layer\n",
        "        output, _ = self.lstm(embedded)\n",
        "\n",
        "        # Extract the final hidden states from both directions\n",
        "        h_n = torch.cat((output[:, -1, :hidden_dim], output[:, 0, hidden_dim:]), dim=1)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        logits = self.fc(h_n)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "FR-Kn5k7HBek"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 128\n",
        "label_size = 3\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_layers = 8\n",
        "dropout_prob = 0.4\n",
        "l1_reg_weight = 0.01\n",
        "learning_rate = 0.001\n",
        "num_epochs = 4\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BetterRNN(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, dropout_prob)\n",
        "model.to(device)  # Move the model to GPU if available\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Create the StepLR scheduler\n",
        "step_size = 2\n",
        "gamma = 0.08\n",
        "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_train_loss = 0.0\n",
        "    total_train_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        inputs, labels = batch.text.to(device), batch.label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average training loss and accuracy for the epoch\n",
        "    average_train_loss = total_train_loss / len(train_iter)\n",
        "    average_train_accuracy = 100 * total_train_accuracy / len(train_data)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_val_loss = 0.0\n",
        "    total_val_accuracy = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_iter:\n",
        "            inputs, labels = batch.text.to(device), batch.label.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average validation loss and accuracy for the epoch\n",
        "    average_val_loss = total_val_loss / len(val_iter)\n",
        "    average_val_accuracy = 100 * total_val_accuracy / len(val_data)\n",
        "\n",
        "    # Print training and validation results for the epoch\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {average_train_loss:.4f}, Train Accuracy: {average_train_accuracy:.2f}%\")\n",
        "    print(f\"Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {average_val_accuracy:.2f}%\")\n",
        "\n",
        "# After training, you cana also evaluate on the test set if needed\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        inputs, labels = batch.text.to(device), batch.label.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_test_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate and print the test accuracy\n",
        "test_accuracy = 100 * total_test_accuracy / len(test_data)\n",
        "print(\"\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "472xd7H3I1io",
        "outputId": "e8c284e7-b49b-4fe4-f638-57fa79f78651"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "Train Loss: 1.0395, Train Accuracy: 45.73%\n",
            "Validation Loss: 1.0190, Validation Accuracy: 48.05%\n",
            "Epoch 2/4\n",
            "Train Loss: 0.9174, Train Accuracy: 58.24%\n",
            "Validation Loss: 0.9372, Validation Accuracy: 58.04%\n",
            "Epoch 3/4\n",
            "Train Loss: 0.7196, Train Accuracy: 69.85%\n",
            "Validation Loss: 0.9870, Validation Accuracy: 58.13%\n",
            "Epoch 4/4\n",
            "Train Loss: 0.5269, Train Accuracy: 78.87%\n",
            "Validation Loss: 1.0595, Validation Accuracy: 58.49%\n",
            "\n",
            "Test Accuracy: 62.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see the Validation and Test Accuracy got increased, Training Set Accuracy Got Decreased as we are using Dropout and L1 Regularization during Training.**"
      ],
      "metadata": {
        "id": "YhytWn1_So6P"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}